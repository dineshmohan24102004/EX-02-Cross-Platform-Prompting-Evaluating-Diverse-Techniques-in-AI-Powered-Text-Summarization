# EX-02-Cross-Platform-Prompting-Evaluating-Diverse-Techniques-in-AI-Powered-Text-Summarization

## AIM
To evaluate and compare the effectiveness of prompting techniques (zero-shot, few-shot, chain-of-thought, role-based) across different AI platforms (e.g., ChatGPT, Gemini, Claude, Copilot) in a specific task: text summarization.

## Introduction 

This evaluation compares five major AI platforms—ChatGPT, Claude, Bard, Cohere Command, and Meta—based on their performance in summarizing complex texts and answering technical questions, key tasks for customer support chatbots. The study assesses each platform’s speed, accuracy, user experience, and response quality, offering practical insights backed by interface visuals and detailed performance metrics.

# Methodology and Experiment Setup
This study employed a structured methodology to evaluate the prompting tools of five AI platforms—ChatGPT, Claude, Bard, Cohere Command, and Meta—within the context of an AI-powered retail customer support chatbot. The experiment focused on two core tasks: summarizing customer inquiries and answering technical questions. Each platform was provided with carefully crafted prompts designed to test its ability to generate accurate, coherent, and contextually relevant responses.

Prompts incorporated a variety of prompting techniques including explicit instruction, context provision, and example-based guidance to assess each system’s flexibility and adaptability. For instance, summarization prompts included realistic customer messages of varied length and complexity, while technical question prompts covered common retail product issues and troubleshooting scenarios.

The following criteria were used for performance evaluation:
•	Accuracy: Correctness of factual information and adequacy of response content.
•	Response Time: Latency from prompt submission to completion of response.
•	Coherence: Logical flow and clarity of the generated output.
•	User Experience Metrics: Including ease of prompt formulation, observed system flexibility, and intuitiveness of platform interfaces.
To ensure objectivity, each AI platform was tested with identical prompt sets under controlled experimental conditions. 
Comparative Analysis of AI Platforms: 
Performance and Response Quality

The evaluation across the five AI platforms—ChatGPT, Claude, Bard, Cohere Command, and Meta—revealed distinct differences in accuracy, summarization quality, and technical question answering capabilities relevant to customer support chatbot development. Each platform was scored on a scale from 1 to 5 for key metrics: accuracy, coherence, response time, and user experience.

## Qualitative Observations

# ChatGPT
       demonstrated the highest overall response quality, consistently delivering concise, accurate summaries and technically sound answers. Its language understanding ensured highly coherent replies, preserving contextual nuance in customer inquiries. Users highlighted its intuitive interface and flexible prompting, contributing to strong user experience ratings
.
# Claude 
       closely followed, showing robust summarization ability and balanced technical accuracy. Its responses were clear and well-structured, with slight variability in handling complex, multi-part questions. Interface usability was rated positively, though some users noted occasional rigidity in prompt interpretations.

# Bard
      exhibited competitive performance in coherence and response speed, but occasionally produced less in-depth technical explanations, leading to moderate scores in accuracy. Its conversational tone was generally well-received, yet response consistency required improvement.

# Cohere
       Command showed strength in technical question answering but lagged behind in summarization precision. Responses were straightforward but sometimes lacked fluent transitions, impacting coherence. User feedback suggested that the interface could benefit from more prompt flexibility.

# Meta
        trailed on most metrics, with slower response times and less accurate output in summarization and technical queries. While its basic language understanding covered general inquiries adequately, responses occasionally suffered from incomplete information and lower coherence levels. User experience was rated lowest due to a more complex interaction workflow.

## Visual Comparison of Responses

The following image illustrates side-by-side example outputs from ChatGPT and Bard summarizing a typical customer inquiry, showcasing differences in brevity, clarity, and detail.
Additionally, Figure 1 below presents a bar chart comparing average accuracy scores and response times for all platforms, highlighting ChatGPT's leading performance and Meta's slower response.
User Experience Evaluation Across Platforms

The user experience for designing an AI-powered customer support chatbot varied significantly across the evaluated platforms. Key factors assessed included interface usability, prompt customization, and response speed, all critical to effective real-world deployment.
Interface Usability and Customization

ChatGPT featured an intuitive interface with clear navigation and flexible prompt input fields, enabling users to easily tailor queries. Similarly, Claude offered a clean, minimalistic layout with some restrictions on prompt structuring, while Bard prioritized conversational flow but with limited advanced customization options. Cohere Command's interface was functional but less user-friendly, requiring additional steps to configure prompts. Meta's platform presented the most complex layout, often disrupting user workflow with dense menus and delayed feedback.


## Response Speed and Interaction Flow
Response latency was generally fastest on ChatGPT and Bard platforms, enhancing fluid user interactions, while Cohere Command and Meta exhibited perceptible delays, potentially impeding real-time chatbot responsiveness. Smooth interaction flow, supported by live preview and prompt suggestion features, was a notable advantage on ChatGPT and Claude interfaces, simplifying iterative prompt refinement.


## Conclusion 
This study shows that ChatGPT is the best overall choice for customer support chatbots due to its strong accuracy and user experience. Claude is a good all-round option, while Bard is fastest but less accurate with technical content. Cohere Command and Meta have potential but need improvement. Choosing the right platform depends on specific needs, and future work should focus on better prompting methods and more varied test scenarios to improve results.


## Result
Thus, the evaluation of 2024 prompting tools across leading AI platforms- ChatGpt , Claude , Bard , Cohere Command, and Meta’s based models has been analysed.


